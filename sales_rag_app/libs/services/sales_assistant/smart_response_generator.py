#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Smart Response Generation Engine with Semantic Understanding

This engine generates intelligent, immediate responses based on semantic query classification,
eliminating the need for clarification requests.
"""

import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from .semantic_query_classifier import QueryResult, SemanticQueryClassifier


@dataclass
class SmartResponse:
    """Structured response from smart generation engine"""
    response_type: str
    immediate_answer: str
    recommendation_summary: str
    target_models: List[str]
    priority_specs: List[str]
    helpful_suggestions: List[str]
    confidence_level: str
    needs_data_lookup: bool
    lookup_strategy: str


class SmartResponseGenerator:
    """
    Smart Response Generation Engine
    
    Generates immediate, helpful responses without clarification requests.
    Uses semantic understanding to provide contextual recommendations.
    """
    
    def __init__(self):
        """Initialize the response generator"""
        self.semantic_classifier = SemanticQueryClassifier()
        
        # Response templates for different strategies
        self.response_templates = {
            "battery_comparison": {
                "immediate_answer": "æ ¹æ“šé›»æ± çºŒèˆªéœ€æ±‚ï¼Œæˆ‘ä¾†ç‚ºæ‚¨æ¨è–¦æœ€é©åˆçš„ç­†é›»å‹è™Ÿï¼š",
                "focus_message": "ğŸ”‹ ä»¥ä¸‹æ˜¯çºŒèˆªèƒ½åŠ›å‡ºè‰²çš„å‹è™Ÿæ¯”è¼ƒï¼š",
                "recommendation_strategy": "highlight_battery_specs"
            },
            
            "performance_comparison": {
                "immediate_answer": "æ ¹æ“šé«˜æ•ˆèƒ½éœ€æ±‚ï¼Œæˆ‘ä¾†ç‚ºæ‚¨åˆ†æé©åˆçš„éŠæˆ²ç­†é›»ï¼š",
                "focus_message": "ğŸ® ä»¥ä¸‹æ˜¯é«˜æ•ˆèƒ½å‹è™Ÿçš„è©³ç´°æ¯”è¼ƒï¼š",
                "recommendation_strategy": "highlight_performance_specs"
            },
            
            "business_features": {
                "immediate_answer": "æ ¹æ“šå•†å‹™è¾¦å…¬éœ€æ±‚ï¼Œæˆ‘ä¾†æ¨è–¦æœ€é©åˆçš„å°ˆæ¥­å‹è™Ÿï¼š",
                "focus_message": "ğŸ’¼ ä»¥ä¸‹æ˜¯å•†å‹™å‹ç­†é›»çš„ç‰¹è‰²æ¯”è¼ƒï¼š",
                "recommendation_strategy": "highlight_business_specs"
            },
            
            "value_comparison": {
                "immediate_answer": "æ ¹æ“šé ç®—å’Œæ€§åƒ¹æ¯”è€ƒé‡ï¼Œæˆ‘ä¾†ç‚ºæ‚¨æ¨è–¦æœ€åˆ’ç®—çš„é¸æ“‡ï¼š",
                "focus_message": "ğŸ’° ä»¥ä¸‹æ˜¯é«˜æ€§åƒ¹æ¯”å‹è™Ÿçš„æ¯”è¼ƒåˆ†æï¼š",
                "recommendation_strategy": "highlight_value_specs"
            },
            
            "display_comparison": {
                "immediate_answer": "æ ¹æ“šè¢å¹•é¡¯ç¤ºéœ€æ±‚ï¼Œæˆ‘ä¾†ç‚ºæ‚¨åˆ†æå„å‹è™Ÿçš„é¡¯ç¤ºæ•ˆæœï¼š",
                "focus_message": "ğŸ–¥ï¸ ä»¥ä¸‹æ˜¯è¢å¹•è¦æ ¼çš„è©³ç´°æ¯”è¼ƒï¼š",
                "recommendation_strategy": "highlight_display_specs"
            },
            
            "general_comparison": {
                "immediate_answer": "æ ¹æ“šæ‚¨çš„æŸ¥è©¢ï¼Œæˆ‘ä¾†ç‚ºæ‚¨æä¾›å…¨é¢çš„å‹è™Ÿæ¯”è¼ƒï¼š",
                "focus_message": "ğŸ“Š ä»¥ä¸‹æ˜¯å„å‹è™Ÿçš„ç¶œåˆæ¯”è¼ƒåˆ†æï¼š",
                "recommendation_strategy": "comprehensive_comparison"
            },
            
            "detailed_specs": {
                "immediate_answer": "æˆ‘ä¾†ç‚ºæ‚¨æä¾›è©³ç´°çš„è¦æ ¼è³‡è¨Šï¼š",
                "focus_message": "ğŸ“‹ ä»¥ä¸‹æ˜¯å®Œæ•´çš„è¦æ ¼å°ç…§è¡¨ï¼š",
                "recommendation_strategy": "detailed_specification_table"
            },
            
            "side_by_side_comparison": {
                "immediate_answer": "æˆ‘ä¾†ç‚ºæ‚¨é€²è¡Œè©³ç´°çš„å‹è™Ÿå°æ¯”åˆ†æï¼š",
                "focus_message": "âš–ï¸ ä»¥ä¸‹æ˜¯ä¸¦åˆ—æ¯”è¼ƒçµæœï¼š",
                "recommendation_strategy": "side_by_side_analysis"
            }
        }
        
        # Model-specific talking points
        self.model_highlights = {
            "819": {
                "strengths": ["è¶…é•·çºŒèˆª8-10å°æ™‚", "è¼•è–„ä¾¿æ”œè¨­è¨ˆ", "å•†å‹™è¾¦å…¬æœ€ä½³é¸æ“‡", "é«˜æ€§åƒ¹æ¯”"],
                "best_for": ["å•†å‹™äººå£«", "å­¸ç”Ÿ", "ç§»å‹•è¾¦å…¬", "é•·æ™‚é–“ä½¿ç”¨"],
                "key_specs": ["battery", "cpu", "memory", "keyboard"]
            },
            "839": {
                "strengths": ["å‡è¡¡é…ç½®", "é©ä¸­åƒ¹ä½", "æ—¥å¸¸ä½¿ç”¨ç©©å®š", "å¤šåŠŸèƒ½æ€§"],
                "best_for": ["ä¸€èˆ¬ç”¨æˆ¶", "å­¸ç”Ÿ", "è¼•åº¦å·¥ä½œ", "å¨›æ¨‚éœ€æ±‚"],
                "key_specs": ["cpu", "memory", "storage", "lcd"]
            },
            "958": {
                "strengths": ["é«˜æ•ˆèƒ½è™•ç†å™¨", "å¼·å‹é¡¯å¡", "éŠæˆ²æµæš¢é«”é©—", "å‰µä½œè€…å‹å¥½"],
                "best_for": ["éŠæˆ²ç©å®¶", "å‰µä½œè€…", "é«˜æ•ˆèƒ½éœ€æ±‚", "å°ˆæ¥­æ‡‰ç”¨"],
                "key_specs": ["gpu", "cpu", "memory", "thermal"]
            }
        }
    
    def generate_smart_response(self, query: str) -> SmartResponse:
        """
        Generate intelligent response based on semantic understanding
        
        Args:
            query: User query string
            
        Returns:
            SmartResponse with immediate helpful content
        """
        try:
            # Classify query semantically
            query_result = self.semantic_classifier.classify_query(query)
            
            # Generate immediate response
            response = self._create_immediate_response(query, query_result)
            
            return response
            
        except Exception as e:
            logging.error(f"Error generating smart response: {e}")
            return self._create_fallback_response(query)
    
    def _create_immediate_response(self, query: str, query_result: QueryResult) -> SmartResponse:
        """Create immediate response based on semantic classification"""
        
        # Get response template
        template = self.response_templates.get(
            query_result.response_strategy, 
            self.response_templates["general_comparison"]
        )
        
        # Generate immediate answer
        immediate_answer = template["immediate_answer"]
        
        # Create recommendation summary
        recommendation_summary = self._generate_recommendation_summary(query_result)
        
        # Generate helpful suggestions
        helpful_suggestions = self._generate_helpful_suggestions(query, query_result)
        
        # Determine confidence level
        confidence_level = self._determine_confidence_level(query_result.confidence)
        
        # Determine data lookup strategy
        lookup_strategy = self._determine_lookup_strategy(query_result)
        
        return SmartResponse(
            response_type=query_result.response_strategy,
            immediate_answer=immediate_answer,
            recommendation_summary=recommendation_summary,
            target_models=query_result.target_models,
            priority_specs=query_result.priority_specs,
            helpful_suggestions=helpful_suggestions,
            confidence_level=confidence_level,
            needs_data_lookup=True,  # Always provide data-backed responses
            lookup_strategy=lookup_strategy
        )
    
    def _generate_recommendation_summary(self, query_result: QueryResult) -> str:
        """Generate contextual recommendation summary"""
        
        category = query_result.query_category
        models = query_result.target_models
        
        # Category-specific recommendations
        category_recommendations = {
            "battery_power": f"ğŸ”‹ æ¨è–¦ {', '.join(models)} ç³»åˆ—ï¼ŒçºŒèˆªèƒ½åŠ›å„ªç•°ï¼Œé©åˆé•·æ™‚é–“ä½¿ç”¨",
            "gaming_performance": f"ğŸ® æ¨è–¦ {', '.join(models)} ç³»åˆ—ï¼Œé«˜æ•ˆèƒ½é…ç½®ï¼ŒéŠæˆ²é«”é©—æµæš¢",
            "business_productivity": f"ğŸ’¼ æ¨è–¦ {', '.join(models)} ç³»åˆ—ï¼Œå•†å‹™åŠŸèƒ½å®Œæ•´ï¼Œç©©å®šå¯é ",
            "student_budget": f"ğŸ“ æ¨è–¦ {', '.join(models)} ç³»åˆ—ï¼Œæ€§åƒ¹æ¯”é«˜ï¼Œé©åˆå­¸ç”Ÿä½¿ç”¨",
            "display_quality": f"ğŸ–¥ï¸ æ¨è–¦ {', '.join(models)} ç³»åˆ—ï¼Œè¢å¹•é¡¯ç¤ºæ•ˆæœå„ªç§€",
            "general_recommendation": f"ğŸ“Š æ¨è–¦ {', '.join(models)} ç³»åˆ—ï¼Œç¶œåˆè¡¨ç¾å„ªç•°",
            "specifications_inquiry": f"ğŸ“‹ æä¾› {', '.join(models)} ç³»åˆ—çš„å®Œæ•´è¦æ ¼è³‡è¨Š",
            "comparison_request": f"âš–ï¸ æä¾› {', '.join(models)} ç³»åˆ—çš„è©³ç´°æ¯”è¼ƒåˆ†æ"
        }
        
        base_recommendation = category_recommendations.get(
            category, 
            f"ğŸ“Œ æ¨è–¦ {', '.join(models)} ç³»åˆ—ï¼Œæ ¹æ“šæ‚¨çš„éœ€æ±‚é€²è¡Œåˆ†æ"
        )
        
        # Add model-specific highlights
        model_details = []
        for model in models[:2]:  # Focus on top 2 models
            if model in self.model_highlights:
                strengths = self.model_highlights[model]["strengths"][:2]  # Top 2 strengths
                detail = f"{model}ç³»åˆ—ï¼š{', '.join(strengths)}"
                model_details.append(detail)
        
        if model_details:
            base_recommendation += f"\\n\\nâœ¨ ç‰¹è‰²äº®é»ï¼š\\n" + "\\n".join([f"â€¢ {detail}" for detail in model_details])
        
        return base_recommendation
    
    def _generate_helpful_suggestions(self, query: str, query_result: QueryResult) -> List[str]:
        """Generate helpful suggestions based on context"""
        
        suggestions = []
        category = query_result.query_category
        models = query_result.target_models
        
        # Category-specific suggestions
        if category == "battery_power":
            suggestions = [
                "è€ƒæ…®æ‚¨çš„å¯¦éš›ä½¿ç”¨æ™‚é–“éœ€æ±‚ï¼ˆ4-6å°æ™‚ vs 8-10å°æ™‚ï¼‰",
                "å»ºè­°äº†è§£å……é›»ç¿’æ…£å’Œä½¿ç”¨ç’°å¢ƒ",
                "çœé›»æ¨¡å¼è¨­å®šå¯å»¶é•·2-3å°æ™‚çºŒèˆª"
            ]
        elif category == "gaming_performance":
            suggestions = [
                "ç¢ºèªæ‚¨ä¸»è¦ç©çš„éŠæˆ²é¡å‹å’Œç•«è³ªè¦æ±‚",
                "è€ƒæ…®æ˜¯å¦éœ€è¦æ”¯æ´VRæˆ–4KéŠæˆ²",
                "æ•£ç†±æ•ˆèƒ½æœƒå½±éŸ¿é•·æ™‚é–“éŠæˆ²çš„ç©©å®šæ€§"
            ]
        elif category == "business_productivity":
            suggestions = [
                "è©•ä¼°æ˜¯å¦éœ€è¦ç¶“å¸¸æ”œå¸¶å¤–å‡ºä½¿ç”¨",
                "è€ƒæ…®ä¼æ¥­å®‰å…¨éœ€æ±‚ï¼ˆå¦‚TPMã€æŒ‡ç´‹è¾¨è­˜ï¼‰",
                "éµç›¤èˆ’é©åº¦å°é•·æ™‚é–“æ‰“å­—å¾ˆé‡è¦"
            ]
        elif category == "student_budget":
            suggestions = [
                "å»ºè­°é—œæ³¨ä¿å›ºæœŸé™å’Œå”®å¾Œæœå‹™",
                "è€ƒæ…®æ˜¯å¦éœ€è¦å‡ç´šç©ºé–“ï¼ˆè¨˜æ†¶é«”ã€ç¡¬ç¢Ÿï¼‰",
                "å­¸ç”Ÿå„ªæƒ æ–¹æ¡ˆå¯ç¯€çœé¡å¤–è²»ç”¨"
            ]
        else:
            # General suggestions
            suggestions = [
                f"å¯é€²ä¸€æ­¥æ¯”è¼ƒ {', '.join(models)} ç³»åˆ—çš„å…·é«”å·®ç•°",
                "å»ºè­°è€ƒæ…®æ‚¨çš„é ç®—ç¯„åœå’Œä½¿ç”¨éœ€æ±‚",
                "æ­¡è¿è©¢å•ä»»ä½•ç‰¹å®šè¦æ ¼çš„è©³ç´°è³‡è¨Š"
            ]
        
        # Add model-specific suggestions
        if len(models) > 1:
            suggestions.append(f"å¦‚éœ€æ·±å…¥æ¯”è¼ƒ {' vs '.join(models[:2])}ï¼Œæˆ‘å¯æä¾›è©³ç´°åˆ†æ")
        
        return suggestions[:4]  # Limit to 4 suggestions
    
    def _determine_confidence_level(self, confidence: float) -> str:
        """Determine confidence level description"""
        if confidence >= 0.7:
            return "é«˜ä¿¡å¿ƒåº¦"
        elif confidence >= 0.4:
            return "ä¸­ç­‰ä¿¡å¿ƒåº¦"
        else:
            return "åŸºç¤æ¨è–¦"
    
    def _determine_lookup_strategy(self, query_result: QueryResult) -> str:
        """Determine the best data lookup strategy"""
        
        strategy_mapping = {
            "battery_comparison": "focus_battery_specs",
            "performance_comparison": "focus_performance_specs",
            "business_features": "focus_business_specs",
            "value_comparison": "focus_value_specs",
            "display_comparison": "focus_display_specs",
            "general_comparison": "comprehensive_lookup",
            "detailed_specs": "complete_specifications",
            "side_by_side_comparison": "parallel_comparison"
        }
        
        return strategy_mapping.get(query_result.response_strategy, "comprehensive_lookup")
    
    def _create_fallback_response(self, query: str) -> SmartResponse:
        """Create fallback response when classification fails"""
        return SmartResponse(
            response_type="general_fallback",
            immediate_answer="æˆ‘ä¾†ç‚ºæ‚¨æä¾›ç­†é›»æ¨è–¦å’Œæ¯”è¼ƒåˆ†æï¼š",
            recommendation_summary="ğŸ“Š æ ¹æ“šæ‚¨çš„æŸ¥è©¢ï¼Œæˆ‘å°‡æä¾›å…¨ç³»åˆ—å‹è™Ÿçš„ç¶œåˆåˆ†æï¼Œå¹«åŠ©æ‚¨æ‰¾åˆ°æœ€é©åˆçš„é¸æ“‡",
            target_models=["819", "839", "958"],
            priority_specs=["cpu", "gpu", "memory", "battery"],
            helpful_suggestions=[
                "æ‚¨å¯ä»¥å‘Šè¨´æˆ‘å…·é«”çš„ä½¿ç”¨éœ€æ±‚ï¼ˆå¦‚éŠæˆ²ã€è¾¦å…¬ã€å­¸ç¿’ï¼‰",
                "æˆ‘å¯ä»¥è©³ç´°æ¯”è¼ƒä»»æ„å…©å€‹å‹è™Ÿçš„å·®ç•°",
                "æ­¡è¿è©¢å•ä»»ä½•è¦æ ¼çš„è©³ç´°è³‡è¨Š"
            ],
            confidence_level="åŸºç¤æ¨è–¦",
            needs_data_lookup=True,
            lookup_strategy="comprehensive_lookup"
        )
    
    def get_context_for_llm(self, smart_response: SmartResponse, query: str) -> Dict[str, Any]:
        """
        Generate context information for LLM prompt construction
        
        This provides structured information that can be used to build
        enhanced prompts for the existing LLM system.
        """
        return {
            "enhanced_prompt_context": {
                "user_intent_analysis": {
                    "detected_category": smart_response.response_type,
                    "confidence": smart_response.confidence_level,
                    "immediate_response": smart_response.immediate_answer
                },
                "recommendation_guidance": {
                    "focus_models": smart_response.target_models,
                    "priority_specs": smart_response.priority_specs,
                    "presentation_strategy": smart_response.lookup_strategy
                },
                "response_enhancement": {
                    "summary_suggestion": smart_response.recommendation_summary,
                    "additional_suggestions": smart_response.helpful_suggestions,
                    "should_clarify": False  # This system never requests clarification
                }
            },
            "user_query": query,
            "response_type": smart_response.response_type
        }


def test_smart_response_generator():
    """Test the smart response generator"""
    generator = SmartResponseGenerator()
    
    test_queries = [
        "å“ªæ¬¾ç­†é›»æ¯”è¼ƒçœé›»ï¼Ÿ",
        "æ¨è–¦é©åˆéŠæˆ²çš„",
        "å­¸ç”Ÿç”¨ä»€éº¼å¥½ï¼Ÿ",
        "æœ‰ä»€éº¼æ¨è–¦çš„å—ï¼Ÿ"
    ]
    
    for query in test_queries:
        print(f"\\n{'='*60}")
        print(f"æŸ¥è©¢: {query}")
        print(f"{'='*60}")
        
        response = generator.generate_smart_response(query)
        
        print(f"å›æ‡‰é¡å‹: {response.response_type}")
        print(f"å³æ™‚å›ç­”: {response.immediate_answer}")
        print(f"æ¨è–¦ç¸½çµ: {response.recommendation_summary}")
        print(f"ç›®æ¨™å‹è™Ÿ: {response.target_models}")
        print(f"å„ªå…ˆè¦æ ¼: {response.priority_specs}")
        print(f"æœ‰ç”¨å»ºè­°: {response.helpful_suggestions}")
        print(f"ä¿¡å¿ƒåº¦: {response.confidence_level}")
        
        # Test LLM context generation
        llm_context = generator.get_context_for_llm(response, query)
        print(f"\\nLLMæ•´åˆè³‡è¨Š: {llm_context['enhanced_prompt_context']['user_intent_analysis']}")


if __name__ == "__main__":
    test_smart_response_generator()