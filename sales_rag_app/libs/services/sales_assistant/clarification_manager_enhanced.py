#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼·ç‰ˆæ¾„æ¸…å°è©±ç®¡ç†å™¨
å¤§å¹…æ¸›å°‘æ¾„æ¸…å°è©±éœ€æ±‚ï¼Œå„ªå…ˆæä¾›å³æ™‚æœ‰ç”¨å›æ‡‰
"""

import json
import uuid
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

@dataclass
class ConversationState:
    """å°è©±ç‹€æ…‹"""
    conversation_id: str
    user_query: str
    current_step: int
    total_steps: int
    clarification_history: List[Dict]
    collected_context: Dict
    confidence_threshold: float
    flow_type: str
    created_at: str
    updated_at: str

@dataclass
class ClarificationQuestion:
    """æ¾„æ¸…å•é¡Œ"""
    question_id: str
    template_name: str
    question: str
    question_type: str
    options: List[Dict]
    conversation_id: str
    step: int

@dataclass 
class ClarificationResponse:
    """æ¾„æ¸…å›æ‡‰"""
    response_id: str
    question_id: str
    user_choice: str
    user_input: str
    conversation_id: str
    timestamp: str

class EnhancedClarificationManager:
    """å¢å¼·ç‰ˆæ¾„æ¸…å°è©±ç®¡ç†å™¨ - æœ€å°åŒ–æ¾„æ¸…ï¼Œæœ€å¤§åŒ–å³æ™‚å›æ‡‰"""
    
    def __init__(self, templates_path: str = None):
        """
        åˆå§‹åŒ–å¢å¼·ç‰ˆæ¾„æ¸…å°è©±ç®¡ç†å™¨
        
        Args:
            templates_path: æ¾„æ¸…å•é¡Œæ¨¡æ¿æ–‡ä»¶è·¯å¾‘
        """
        self.templates_path = templates_path or "sales_rag_app/libs/services/sales_assistant/prompts/clarification_templates.json"
        self.clarification_templates = self._load_clarification_templates()
        self.active_conversations: Dict[str, ConversationState] = {}
        
        # å¤§å¹…æé«˜ä¿¡å¿ƒåº¦é–¾å€¼ï¼Œæ¸›å°‘æ¾„æ¸…è§¸ç™¼
        self.confidence_threshold = 0.15  # å¾ 0.6 é™åˆ° 0.15
        self.min_clarification_threshold = 0.05  # åªæœ‰æ¥µä½ä¿¡å¿ƒåº¦æ‰æ¾„æ¸…
        
        logging.info("å¢å¼·ç‰ˆæ¾„æ¸…å°è©±ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ - æœ€å°åŒ–æ¾„æ¸…æ¨¡å¼")
    
    def _load_clarification_templates(self) -> Dict:
        """è¼‰å…¥æ¾„æ¸…å•é¡Œæ¨¡æ¿"""
        try:
            with open(self.templates_path, 'r', encoding='utf-8') as f:
                templates = json.load(f)
                logging.info(f"æˆåŠŸè¼‰å…¥æ¾„æ¸…æ¨¡æ¿: {list(templates.get('clarification_templates', {}).keys())}")
                return templates
        except FileNotFoundError:
            logging.warning(f"æ¾„æ¸…æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {self.templates_path}")
            return self._get_minimal_templates()
        except Exception as e:
            logging.error(f"è¼‰å…¥æ¾„æ¸…æ¨¡æ¿å¤±æ•—: {e}")
            return self._get_minimal_templates()
    
    def _get_minimal_templates(self) -> Dict:
        """ç²å–æœ€å°æ¾„æ¸…æ¨¡æ¿ï¼ˆå¾ˆå°‘ä½¿ç”¨ï¼‰"""
        return {
            "clarification_templates": {
                "extreme_ambiguity": {
                    "question": "æ‚¨çš„æŸ¥è©¢æ¯”è¼ƒç°¡çŸ­ï¼Œç‚ºäº†æä¾›æ›´æº–ç¢ºçš„å»ºè­°ï¼Œè«‹å•æ‚¨ä¸»è¦é—œå¿ƒä»€éº¼æ–¹é¢ï¼Ÿ",
                    "options": [
                        {"id": "performance", "label": "ğŸš€ æ€§èƒ½è¡¨ç¾"},
                        {"id": "battery", "label": "ğŸ”‹ é›»æ± çºŒèˆª"},
                        {"id": "portability", "label": "ğŸ’ è¼•ä¾¿æ”œå¸¶"},
                        {"id": "general", "label": "ğŸ“‹ ç¶œåˆæ¯”è¼ƒ"}
                    ]
                }
            }
        }
    
    def should_clarify_enhanced(self, intent_result: Dict, smart_context: Dict = None) -> bool:
        """
        å¢å¼·ç‰ˆæ¾„æ¸…åˆ¤æ–· - å¤§å¹…æ¸›å°‘æ¾„æ¸…éœ€æ±‚
        
        Args:
            intent_result: æ„åœ–æª¢æ¸¬çµæœ
            smart_context: æ™ºèƒ½å›æ‡‰ä¸Šä¸‹æ–‡
            
        Returns:
            æ˜¯å¦éœ€è¦æ¾„æ¸…
        """
        try:
            confidence = intent_result.get("confidence_score", 0.0)
            primary_intent = intent_result.get("primary_intent", "general")
            matched_keywords = intent_result.get("matched_keywords", [])
            
            # 1. å¦‚æœæœ‰æ˜ç¢ºçš„æ„åœ–ï¼Œä¸éœ€è¦æ¾„æ¸…
            if confidence > self.confidence_threshold:
                logging.info(f"ä¿¡å¿ƒåº¦è¶³å¤  ({confidence:.3f} > {self.confidence_threshold})ï¼Œä¸éœ€è¦æ¾„æ¸…")
                return False
            
            # 2. å¦‚æœæœ‰ä»»ä½•åŒ¹é…çš„é—œéµå­—ï¼Œä¸éœ€è¦æ¾„æ¸…
            if matched_keywords:
                logging.info(f"æœ‰åŒ¹é…é—œéµå­— ({len(matched_keywords)} å€‹)ï¼Œä¸éœ€è¦æ¾„æ¸…")
                return False
            
            # 3. å¦‚æœæ™ºèƒ½ä¸Šä¸‹æ–‡èƒ½å¤ ç”Ÿæˆæœ‰ç”¨å›æ‡‰ï¼Œä¸éœ€è¦æ¾„æ¸…
            if smart_context:
                response_strategy = smart_context.get("response_strategy", "")
                if response_strategy != "general_recommendation":
                    logging.info(f"æœ‰æ˜ç¢ºå›æ‡‰ç­–ç•¥ ({response_strategy})ï¼Œä¸éœ€è¦æ¾„æ¸…")
                    return False
                
                recommended_models = smart_context.get("recommended_models", [])
                if recommended_models:
                    logging.info(f"æœ‰æ¨è–¦å‹è™Ÿ ({recommended_models})ï¼Œä¸éœ€è¦æ¾„æ¸…")
                    return False
            
            # 4. æª¢æŸ¥æ˜¯å¦ç‚ºæ¥µåº¦æ¨¡ç³Šçš„æŸ¥è©¢
            original_query = smart_context.get("original_query", "") if smart_context else ""
            if self._is_extremely_ambiguous(original_query, intent_result):
                logging.info("æŸ¥è©¢æ¥µåº¦æ¨¡ç³Šï¼Œéœ€è¦æ¾„æ¸…")
                return True
            
            # 5. é è¨­ä¸æ¾„æ¸…ï¼Œæä¾›æœ€ä½³çŒœæ¸¬å›æ‡‰
            logging.info("é è¨­ä¸æ¾„æ¸…ï¼Œå°‡æä¾›åŸºæ–¼æ¨æ–·çš„å›æ‡‰")
            return False
            
        except Exception as e:
            logging.error(f"æ¾„æ¸…åˆ¤æ–·å¤±æ•—: {e}")
            # éŒ¯èª¤æƒ…æ³ä¸‹ä¹Ÿä¸æ¾„æ¸…ï¼Œé¿å…ä¸­æ–·ç”¨æˆ¶é«”é©—
            return False
    
    def _is_extremely_ambiguous(self, query: str, intent_result: Dict) -> bool:
        """
        åˆ¤æ–·æŸ¥è©¢æ˜¯å¦æ¥µåº¦æ¨¡ç³Š
        
        Args:
            query: åŸå§‹æŸ¥è©¢
            intent_result: æ„åœ–æª¢æ¸¬çµæœ
            
        Returns:
            æ˜¯å¦æ¥µåº¦æ¨¡ç³Š
        """
        if not query or len(query.strip()) < 3:
            return True
        
        # æ¥µåº¦ç°¡çŸ­ä¸”ç„¡æ„ç¾©çš„æŸ¥è©¢
        meaningless_queries = [
            "ç­†é›»", "é›»è…¦", "laptop", "æ¨è–¦", "å»ºè­°", "å“ªå€‹", "ä»€éº¼", "æœ‰å—", 
            "å¥½å—", "å¦‚ä½•", "æ€æ¨£", "?", "ï¼Ÿ", "å¹«å¿™", "è¬è¬"
        ]
        
        query_clean = query.strip().lower()
        if query_clean in meaningless_queries:
            return True
        
        # æª¢æŸ¥æ˜¯å¦åªåŒ…å«åœç”¨è©
        stop_words = ["çš„", "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "æˆ–", "ä½†", "å°±", "éƒ½", "å¾ˆ", "éå¸¸", "æ¯”è¼ƒ", "ä¸€äº›"]
        words = query_clean.split()
        meaningful_words = [w for w in words if w not in stop_words and len(w) > 1]
        
        if len(meaningful_words) == 0:
            return True
        
        # æª¢æŸ¥æ„åœ–æª¢æ¸¬çµæœ
        confidence = intent_result.get("confidence_score", 0.0)
        matched_keywords = intent_result.get("matched_keywords", [])
        
        # åªæœ‰åœ¨å®Œå…¨æ²’æœ‰ä»»ä½•ç·šç´¢æ™‚æ‰èªç‚ºæ¥µåº¦æ¨¡ç³Š
        if confidence < self.min_clarification_threshold and len(matched_keywords) == 0:
            return True
        
        return False
    
    def generate_smart_fallback_response(self, query: str, intent_result: Dict, smart_context: Dict = None) -> Dict:
        """
        ç”Ÿæˆæ™ºèƒ½å¾Œå‚™å›æ‡‰ï¼Œé¿å…æ¾„æ¸…
        
        Args:
            query: åŸå§‹æŸ¥è©¢
            intent_result: æ„åœ–æª¢æ¸¬çµæœ
            smart_context: æ™ºèƒ½å›æ‡‰ä¸Šä¸‹æ–‡
            
        Returns:
            æ™ºèƒ½å¾Œå‚™å›æ‡‰
        """
        try:
            response_strategy = smart_context.get("response_strategy", "general_recommendation") if smart_context else "general_recommendation"
            recommended_models = smart_context.get("recommended_models", ["958", "839", "819"]) if smart_context else ["958", "839", "819"]
            priority_specs = smart_context.get("priority_specs", ["cpu", "gpu", "memory"]) if smart_context else ["cpu", "gpu", "memory"]
            
            # æ ¹æ“šç­–ç•¥ç”Ÿæˆå›æ‡‰
            if response_strategy == "comparison":
                return self._generate_comparison_response(recommended_models, priority_specs)
            elif response_strategy == "spec_comparison":
                primary_intent = intent_result.get("primary_intent", "general")
                return self._generate_spec_focused_response(primary_intent, recommended_models)
            elif response_strategy == "latest_products":
                return self._generate_latest_products_response()
            elif response_strategy == "scenario_recommendation":
                return self._generate_scenario_recommendation()
            else:
                return self._generate_general_recommendation_response(recommended_models)
                
        except Exception as e:
            logging.error(f"ç”Ÿæˆæ™ºèƒ½å¾Œå‚™å›æ‡‰å¤±æ•—: {e}")
            return self._generate_default_helpful_response()
    
    def _generate_comparison_response(self, models: List[str], priority_specs: List[str]) -> Dict:
        """ç”Ÿæˆæ¯”è¼ƒå›æ‡‰"""
        return {
            "message_type": "smart_response",
            "response_type": "comparison",
            "answer_summary": f"æ ¹æ“šæ‚¨çš„æŸ¥è©¢ï¼Œä»¥ä¸‹æ˜¯{', '.join(models)}ç³»åˆ—çš„æ¯”è¼ƒåˆ†æï¼š",
            "recommended_action": "show_model_comparison",
            "target_models": models,
            "priority_specs": priority_specs,
            "helpful_context": "æˆ‘å€‘ç‚ºæ‚¨æ•´ç†äº†é‡é»è¦æ ¼æ¯”è¼ƒï¼Œå¦‚éœ€ç‰¹å®šæ–¹é¢çš„è©³ç´°ä¿¡æ¯ï¼Œè«‹å‘Šè¨´æˆ‘å€‘ã€‚",
            "additional_suggestions": [
                "ğŸ”‹ å¦‚æœé‡è¦–çºŒèˆªï¼Œæ¨è–¦819ç³»åˆ—",
                "ğŸ® å¦‚æœç”¨æ–¼éŠæˆ²ï¼Œæ¨è–¦958ç³»åˆ—", 
                "âš–ï¸ å¦‚æœè¦å¹³è¡¡æ€§èƒ½ï¼Œæ¨è–¦839ç³»åˆ—"
            ]
        }
    
    def _generate_spec_focused_response(self, spec_type: str, models: List[str]) -> Dict:
        """ç”Ÿæˆè¦æ ¼å°ˆæ³¨å›æ‡‰"""
        spec_descriptions = {
            "battery": "é›»æ± çºŒèˆª",
            "display": "è¢å¹•é¡¯ç¤º",
            "cpu": "è™•ç†å™¨æ€§èƒ½",
            "gpu": "é¡¯å¡æ€§èƒ½", 
            "memory": "è¨˜æ†¶é«”é…ç½®",
            "storage": "å„²å­˜å®¹é‡",
            "portability": "é‡é‡ä¾¿æ”œæ€§"
        }
        
        spec_name = spec_descriptions.get(spec_type, "ç¶œåˆæ€§èƒ½")
        
        return {
            "message_type": "smart_response",
            "response_type": "spec_focused",
            "answer_summary": f"é—œæ–¼{spec_name}æ–¹é¢ï¼Œä»¥ä¸‹æ˜¯å„ç³»åˆ—çš„è¡¨ç¾åˆ†æï¼š",
            "recommended_action": "show_spec_comparison",
            "focus_spec": spec_type,
            "target_models": models,
            "helpful_context": f"æˆ‘å€‘å·²ç‚ºæ‚¨é‡é»æ•´ç†{spec_name}çš„è©³ç´°æ¯”è¼ƒã€‚",
            "additional_suggestions": self._get_spec_specific_suggestions(spec_type)
        }
    
    def _generate_latest_products_response(self) -> Dict:
        """ç”Ÿæˆæœ€æ–°ç”¢å“å›æ‡‰"""
        return {
            "message_type": "smart_response", 
            "response_type": "latest_products",
            "answer_summary": "ä»¥ä¸‹æ˜¯æˆ‘å€‘ç›®å‰çš„æœ€æ–°ç”¢å“ç³»åˆ—ï¼š",
            "recommended_action": "show_latest_models",
            "target_models": ["958", "839", "819"],
            "helpful_context": "æ‰€æœ‰ç³»åˆ—éƒ½æ˜¯æœ€æ–°é…ç½®ï¼Œå„æœ‰ä¸åŒçš„ç‰¹è‰²å’Œå®šä½ã€‚",
            "additional_suggestions": [
                "ğŸš€ 958ç³»åˆ—ï¼šæœ€æ–°é«˜æ€§èƒ½é…ç½®",
                "âš–ï¸ 839ç³»åˆ—ï¼šæ€§èƒ½èˆ‡åƒ¹æ ¼å¹³è¡¡",
                "ğŸ¢ 819ç³»åˆ—ï¼šå•†å‹™è¾¦å…¬å„ªåŒ–"
            ]
        }
    
    def _generate_scenario_recommendation(self) -> Dict:
        """ç”Ÿæˆä½¿ç”¨å ´æ™¯æ¨è–¦"""
        return {
            "message_type": "smart_response",
            "response_type": "scenario_recommendation", 
            "answer_summary": "æ ¹æ“šä¸åŒä½¿ç”¨å ´æ™¯ï¼Œæˆ‘å€‘ç‚ºæ‚¨æ¨è–¦ä»¥ä¸‹é¸æ“‡ï¼š",
            "recommended_action": "show_scenario_models",
            "target_models": ["958", "839", "819"],
            "helpful_context": "æ¯å€‹ç³»åˆ—éƒ½é‡å°ç‰¹å®šä½¿ç”¨å ´æ™¯é€²è¡Œäº†å„ªåŒ–ã€‚",
            "additional_suggestions": [
                "ğŸ® éŠæˆ²å¨›æ¨‚ï¼š958ç³»åˆ— - é«˜æ€§èƒ½GPUå’ŒCPU",
                "ğŸ’¼ å•†å‹™è¾¦å…¬ï¼š819ç³»åˆ— - é•·çºŒèˆªå’Œè¼•ä¾¿è¨­è¨ˆ",
                "ğŸ“š å­¸ç¿’å‰µä½œï¼š839ç³»åˆ— - æ€§èƒ½èˆ‡ä¾¿æ”œçš„å¹³è¡¡",
                "ğŸ¨ å°ˆæ¥­å‰µä½œï¼š958ç³»åˆ— - å°ˆæ¥­ç´šé¡¯å¡å’Œå¤§è¨˜æ†¶é«”"
            ]
        }
    
    def _generate_general_recommendation_response(self, models: List[str]) -> Dict:
        """ç”Ÿæˆä¸€èˆ¬æ¨è–¦å›æ‡‰"""
        return {
            "message_type": "smart_response",
            "response_type": "general_recommendation",
            "answer_summary": "åŸºæ–¼æˆ‘å€‘çš„ç”¢å“ç‰¹è‰²ï¼Œç‚ºæ‚¨æ¨è–¦ä»¥ä¸‹é¸æ“‡ï¼š",
            "recommended_action": "show_general_comparison",
            "target_models": models,
            "helpful_context": "æˆ‘å€‘ç‚ºæ‚¨æ•´ç†äº†å„ç³»åˆ—çš„ç‰¹è‰²æ¯”è¼ƒï¼Œå¹«åŠ©æ‚¨åšå‡ºæœ€é©åˆçš„é¸æ“‡ã€‚",
            "additional_suggestions": [
                "ğŸ’¡ æ€§èƒ½éœ€æ±‚é«˜ï¼šæ¨è–¦958ç³»åˆ—",
                "ğŸ’° é ç®—è€ƒé‡ï¼šæ¨è–¦839ç³»åˆ—",
                "ğŸ”‹ çºŒèˆªé‡è¦ï¼šæ¨è–¦819ç³»åˆ—",
                "â“ ä¸ç¢ºå®šéœ€æ±‚ï¼šå¯ä»¥çœ‹çœ‹ç¶œåˆæ¯”è¼ƒ"
            ]
        }
    
    def _generate_default_helpful_response(self) -> Dict:
        """ç”Ÿæˆé è¨­æœ‰ç”¨å›æ‡‰"""
        return {
            "message_type": "smart_response",
            "response_type": "helpful_default",
            "answer_summary": "æˆ‘å€‘æœ‰ä¸‰å€‹ä¸»è¦ç³»åˆ—ï¼Œå„æœ‰ä¸åŒç‰¹è‰²ï¼š",
            "recommended_action": "show_all_series",
            "target_models": ["958", "839", "819"],
            "helpful_context": "æ¯å€‹ç³»åˆ—éƒ½æœ‰å…¶ç¨ç‰¹å„ªå‹¢ï¼Œæˆ‘å€‘å¯ä»¥æ ¹æ“šæ‚¨çš„å…·é«”éœ€æ±‚æä¾›è©³ç´°å»ºè­°ã€‚",
            "additional_suggestions": [
                "ğŸš€ 958ç³»åˆ—ï¼šé ‚ç´šæ€§èƒ½ï¼Œé©åˆéŠæˆ²å’Œå°ˆæ¥­æ‡‰ç”¨",
                "âš–ï¸ 839ç³»åˆ—ï¼šå¹³è¡¡é…ç½®ï¼Œé©åˆä¸€èˆ¬å·¥ä½œå’Œè¼•åº¦éŠæˆ²",
                "ğŸ¢ 819ç³»åˆ—ï¼šå•†å‹™å°å‘ï¼Œå„ªç§€çºŒèˆªå’Œä¾¿æ”œæ€§",
                "ğŸ“ å¦‚æœ‰ç‰¹å®šéœ€æ±‚ï¼Œæ­¡è¿é€²ä¸€æ­¥è©¢å•"
            ]
        }
    
    def _get_spec_specific_suggestions(self, spec_type: str) -> List[str]:
        """ç²å–ç‰¹å®šè¦æ ¼çš„å»ºè­°"""
        suggestions_map = {
            "battery": [
                "ğŸ”‹ 819ç³»åˆ—ï¼š8-10å°æ™‚è¶…é•·çºŒèˆª",
                "âš¡ 839ç³»åˆ—ï¼š6-8å°æ™‚å¹³è¡¡çºŒèˆª",
                "ğŸš€ 958ç³»åˆ—ï¼š5-7å°æ™‚é«˜æ€§èƒ½çºŒèˆª"
            ],
            "display": [
                "ğŸ® 958ç³»åˆ—ï¼š144Hzé«˜åˆ·æ–°ç‡ï¼Œé©åˆéŠæˆ²",
                "ğŸ’¼ 819ç³»åˆ—ï¼šè­·çœ¼è¢å¹•ï¼Œé©åˆé•·æ™‚é–“å·¥ä½œ",
                "ğŸ“º 839ç³»åˆ—ï¼šIPSé¢æ¿ï¼Œè‰²å½©æº–ç¢º"
            ],
            "cpu": [
                "ğŸš€ 958ç³»åˆ—ï¼šRyzen 7é«˜æ€§èƒ½è™•ç†å™¨",
                "âš–ï¸ 839ç³»åˆ—ï¼šRyzen 5å¹³è¡¡æ€§èƒ½",
                "ğŸ’¼ 819ç³»åˆ—ï¼šç¯€èƒ½è™•ç†å™¨ï¼ŒçºŒèˆªå„ªåŒ–"
            ],
            "gpu": [
                "ğŸ® 958ç³»åˆ—ï¼šé«˜æ€§èƒ½ç¨ç«‹é¡¯å¡",
                "ğŸ“Š 839ç³»åˆ—ï¼šä¸­éšç¨ç«‹é¡¯å¡", 
                "ğŸ’¼ 819ç³»åˆ—ï¼šæ•´åˆé¡¯å¡ï¼Œçœé›»é«˜æ•ˆ"
            ]
        }
        
        return suggestions_map.get(spec_type, [
            "ğŸ’¡ æ¯å€‹ç³»åˆ—éƒ½æœ‰å…¶ç‰¹è‰²å„ªå‹¢",
            "ğŸ“Š å¯ä»¥æŸ¥çœ‹è©³ç´°è¦æ ¼æ¯”è¼ƒ",
            "ğŸ¤” å¦‚æœ‰ç–‘å•æ­¡è¿é€²ä¸€æ­¥è©¢å•"
        ])
    
    def should_clarify(self, intent_result: Dict, confidence_threshold: float = None) -> bool:
        """
        å‘å¾Œå…¼å®¹çš„æ¾„æ¸…åˆ¤æ–·æ¥å£
        """
        return self.should_clarify_enhanced(intent_result, None)
    
    def start_clarification(self, query: str, intent_result: Dict) -> Tuple[str, ClarificationQuestion]:
        """
        é–‹å§‹æ¾„æ¸…å°è©±ï¼ˆåªåœ¨æ¥µåº¦å¿…è¦æ™‚ä½¿ç”¨ï¼‰
        """
        conversation_id = str(uuid.uuid4())
        
        # å‰µå»ºæ¥µç°¡æ¾„æ¸…å•é¡Œ
        question = ClarificationQuestion(
            question_id=str(uuid.uuid4()),
            template_name="extreme_ambiguity",
            question="æ‚¨å¸Œæœ›äº†è§£ç­†é›»çš„å“ªå€‹æ–¹é¢ï¼Ÿ",
            question_type="single_choice",
            options=[
                {"id": "performance", "label": "ğŸš€ æ€§èƒ½è¡¨ç¾", "description": "CPUã€GPUç­‰æ€§èƒ½è¦æ ¼"},
                {"id": "battery", "label": "ğŸ”‹ é›»æ± çºŒèˆª", "description": "çºŒèˆªæ™‚é–“å’Œçœé›»ç‰¹æ€§"},
                {"id": "portability", "label": "ğŸ’ è¼•ä¾¿æ”œå¸¶", "description": "é‡é‡ã€å°ºå¯¸ç­‰ä¾¿æ”œæ€§"},
                {"id": "general", "label": "ğŸ“‹ ç¶œåˆæ¯”è¼ƒ", "description": "æ•´é«”è¦æ ¼å’Œæ¨è–¦"}
            ],
            conversation_id=conversation_id,
            step=1
        )
        
        # å‰µå»ºå°è©±ç‹€æ…‹
        conversation_state = ConversationState(
            conversation_id=conversation_id,
            user_query=query,
            current_step=1,
            total_steps=1,  # åªä¸€æ­¥æ¾„æ¸…
            clarification_history=[],
            collected_context={},
            confidence_threshold=self.confidence_threshold,
            flow_type="minimal_clarification",
            created_at=datetime.now().isoformat(),
            updated_at=datetime.now().isoformat()
        )
        
        self.active_conversations[conversation_id] = conversation_state
        
        logging.info(f"é–‹å§‹æœ€å°æ¾„æ¸…å°è©±: {conversation_id}")
        return conversation_id, question
    
    def process_clarification_response(self, conversation_id: str, user_choice: str, user_input: str = "") -> Dict:
        """
        è™•ç†æ¾„æ¸…å›æ‡‰ï¼ˆç«‹å³å®Œæˆï¼Œä¸å»¶çºŒå°è©±ï¼‰
        """
        if conversation_id not in self.active_conversations:
            return {"action": "error", "message": "å°è©±ä¸å­˜åœ¨"}
        
        conversation_state = self.active_conversations[conversation_id]
        
        # ç«‹å³å®Œæˆæ¾„æ¸…ï¼Œç”Ÿæˆå¢å¼·æ„åœ–
        enhanced_intent = self._generate_enhanced_intent_from_choice(user_choice, conversation_state.user_query)
        
        # æ¸…ç†å°è©±ç‹€æ…‹
        del self.active_conversations[conversation_id]
        
        return {
            "action": "complete",
            "enhanced_intent": enhanced_intent,
            "clarification_summary": f"ç”¨æˆ¶é—œæ³¨ï¼š{user_choice}",
            "conversation_id": conversation_id
        }
    
    def _generate_enhanced_intent_from_choice(self, choice: str, original_query: str) -> Dict:
        """æ ¹æ“šæ¾„æ¸…é¸æ“‡ç”Ÿæˆå¢å¼·æ„åœ–"""
        choice_mapping = {
            "performance": {
                "primary_intent": "cpu",
                "confidence_score": 0.8,
                "priority_specs": ["cpu", "gpu", "memory"],
                "recommended_models": ["958", "839"]
            },
            "battery": {
                "primary_intent": "battery", 
                "confidence_score": 0.8,
                "priority_specs": ["battery", "cpu"],
                "recommended_models": ["819", "839"]
            },
            "portability": {
                "primary_intent": "portability",
                "confidence_score": 0.8, 
                "priority_specs": ["structconfig", "battery"],
                "recommended_models": ["819"]
            },
            "general": {
                "primary_intent": "comparison",
                "confidence_score": 0.8,
                "priority_specs": ["cpu", "gpu", "memory", "battery"],
                "recommended_models": ["958", "839", "819"]
            }
        }
        
        enhanced = choice_mapping.get(choice, choice_mapping["general"])
        enhanced["original_query"] = original_query
        enhanced["enhanced_by_clarification"] = True
        
        return enhanced