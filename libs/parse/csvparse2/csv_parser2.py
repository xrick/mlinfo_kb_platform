# import pandas as pd
import json
# import re
from typing import Any, Dict, Optional
from pathlib import Path
import csv
# import argparse

import logging
import pandas as pd

# å°å…¥çˆ¶é¡åˆ¥
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from parsebase import ParseBase

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CSVParser2(ParseBase):

    def __init__(self):
        super().__init__()
        self.rawcsv = None
        self.processed_csv = None
        self.datalist = None
        self._rules = None
        self._rules_file = Path(__file__).parent / "rules.json"
        self.headers = None
        self.processed_result = None
        self.default_output_path = None

    def beforeParse(self, data: Any, config: Optional[Dict] = None) -> bool:
        """
        è§£æå‰æº–å‚™å·¥ä½œ
        
        Args:
            data: CSV æª”æ¡ˆè·¯å¾‘æˆ– DataFrame
            config: è§£æé…ç½®åƒæ•¸
            
        Returns:
            bool: æº–å‚™å·¥ä½œæ˜¯å¦æˆåŠŸ
        """
        try:
            # è¨­ç½®è¼¸å…¥è³‡æ–™
            self.rawcsv = data
            #first check if the file is xlsx, if so, convert to csv
            self.rawcsv = self.convert_xlsx_to_csv_if_needed(self.rawcsv)
            # è¼‰å…¥ CSV ä¸¦è‡ªå‹•æª¢æ¸¬æ ¼å¼
            self.datalist = self._load_csv(self.rawcsv)
            
            # è‡ªå‹•æª¢æ¸¬ CSV æ ¼å¼ä¸¦æ±ºå®šè™•ç†æ–¹å¼
            if self._is_structured_specs_csv():
                # ä½¿ç”¨è¦å‰‡é©…å‹•è§£æï¼ˆé©ç”¨æ–¼ç­†é›»è¦æ ¼CSVï¼‰
                logger.info("æª¢æ¸¬åˆ°çµæ§‹åŒ–è¦æ ¼ CSVï¼Œä½¿ç”¨è¦å‰‡é©…å‹•è§£æ")
                self._rules = self._load_rules()
                if not self._rules:
                    logger.error("ç„¡æ³•è¼‰å…¥è§£æè¦å‰‡")
                    return False
                
                # è¨­ç½®æ¨™é¡Œ
                self.headers = [
                    rule.get("column_name", f"æ¬„ä½{i+1}")
                    for i, rule in enumerate(self._rules[1])
                ]
                
                # è¨­ç½®æ¨¡å‹åƒæ•¸
                self.default_output_path = self._rules[0][0].get("default_output_path", "./output.csv")
                
            else:
                # ä½¿ç”¨å‹•æ…‹è§£æï¼ˆé©ç”¨æ–¼ä¸€èˆ¬CSVï¼‰
                logger.info("æª¢æ¸¬åˆ°ä¸€èˆ¬ CSV æ ¼å¼ï¼Œä½¿ç”¨å‹•æ…‹è§£æ")
                self._setup_dynamic_parsing()
            
            logger.info(f"è§£æå‰æº–å‚™å®Œæˆ - æ¨™é¡Œ: {self.headers}")
            return True
            
        except Exception as e:
            logger.error(f"è§£æå‰æº–å‚™å¤±æ•—: {str(e)}")
            return False
         

    def inParse(self):
        """
        ä¸»è¦è§£æé‚è¼¯
        
        Returns:
            List[Dict]: è§£æçµæœåˆ—è¡¨
        """
        if self._rules is None:
            # å‹•æ…‹è§£ææ¨¡å¼
            self._dynamic_collect_results()
        else:
            # è¦å‰‡é©…å‹•è§£ææ¨¡å¼
            self.collect_results()
        
        return self.processed_result

    
    def endParse(self) -> bool:
        """
        è§£æå¾Œè™•ç†å·¥ä½œ
        
        Returns:
            bool: å¾Œè™•ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.processed_result:
                self.write_csv()
                logger.info("è§£æå¾Œè™•ç†å®Œæˆ")
                return True
            else:
                logger.error("ç„¡è§£æçµæœå¯è™•ç†")
                return False
        except Exception as e:
            logger.error(f"è§£æå¾Œè™•ç†å¤±æ•—: {str(e)}")
            return False
        
    
    def _load_rules(self):
        with open(self._rules_file, "r", encoding="utf-8-sig") as f:
            return json.load(f)
        


    def _load_csv(self, file_path):
        with open(file_path, mode='r', encoding='utf-8-sig') as f:
            return list(csv.reader(f))
    
    def _is_structured_specs_csv(self) -> bool:
        """
        æª¢æ¸¬æ˜¯å¦ç‚ºçµæ§‹åŒ–è¦æ ¼ CSVï¼ˆå¦‚ raw_938.csv æ ¼å¼ï¼‰
        
        Returns:
            bool: æ˜¯å¦ç‚ºçµæ§‹åŒ–è¦æ ¼æ ¼å¼
        """
        if not self.datalist or len(self.datalist) < 3:
            return False
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«ç­†é›»è¦æ ¼çš„é—œéµè©
        spec_keywords = ["Model", "Model Name", "CPU", "GPU", "Memory", "LCD", "Battery", "Stage", "Version"]
        
        # æª¢æŸ¥å‰å¹¾è¡Œæ˜¯å¦åŒ…å«é€™äº›é—œéµè©
        content_text = " ".join([" ".join(row) for row in self.datalist[:10]])
        
        keyword_count = sum(1 for keyword in spec_keywords if keyword in content_text)
        
        # å¦‚æœåŒ…å«3å€‹ä»¥ä¸Šçš„è¦æ ¼é—œéµè©ï¼Œèªç‚ºæ˜¯çµæ§‹åŒ–è¦æ ¼CSV
        return keyword_count >= 3
    
    def _setup_dynamic_parsing(self):
        """
        è¨­ç½®å‹•æ…‹è§£ææ¨¡å¼ï¼ˆé©ç”¨æ–¼ä¸€èˆ¬CSVï¼‰
        """
        if not self.datalist:
            raise Exception("CSV è³‡æ–™ç‚ºç©º")
        
        # ä½¿ç”¨ç¬¬ä¸€è¡Œä½œç‚ºæ¨™é¡Œè¡Œ
        if len(self.datalist) > 0:
            self.headers = self.datalist[0]
            # ç§»é™¤æ¨™é¡Œè¡Œ
            self.datalist = self.datalist[1:]
        else:
            self.headers = []
        
        # è¨­ç½®å‹•æ…‹åƒæ•¸
        self.default_output_path = "./output.csv"
        
        # å‰µå»ºå‹•æ…‹è¦å‰‡ï¼ˆä¸é€²è¡Œè¦å‰‡åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹è³‡æ–™ï¼‰
        self._rules = None
        
        logger.info(f"å‹•æ…‹è§£æè¨­å®š - æ¨™é¡Œ: {self.headers}, è³‡æ–™è¡Œæ•¸: {len(self.datalist)}")

    def _dynamic_collect_results(self):
        """
        å‹•æ…‹æ”¶é›†çµæœï¼ˆé©ç”¨æ–¼ä¸€èˆ¬CSVï¼‰
        """
        self.processed_result = []
        valid_rows = 0
        empty_rows_skipped = 0
        
        for row in self.datalist:
            # æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆè¡Œï¼šè‡³å°‘åŒ…å«ä¸€å€‹éç©ºå€¼
            if self._is_valid_row(row):
                # ç¢ºä¿è¡Œè³‡æ–™é•·åº¦èˆ‡æ¨™é¡ŒåŒ¹é…
                padded_row = row + [''] * (len(self.headers) - len(row))
                self.processed_result.append(padded_row[:len(self.headers)])
                valid_rows += 1
            else:
                empty_rows_skipped += 1
        
        logger.info(f"å‹•æ…‹è§£æå®Œæˆ - è™•ç†äº† {valid_rows} è¡Œæœ‰æ•ˆè³‡æ–™ï¼Œè·³é {empty_rows_skipped} è¡Œç©ºè³‡æ–™")

    def _is_valid_row(self, row):
        """
        æª¢æŸ¥è¡Œæ˜¯å¦åŒ…å«æœ‰æ•ˆè³‡æ–™
        
        Args:
            row: CSV è¡Œè³‡æ–™åˆ—è¡¨
            
        Returns:
            bool: æ˜¯å¦ç‚ºæœ‰æ•ˆè¡Œ
        """
        if not row:
            return False
        
        # æª¢æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€å€‹éç©ºå€¼
        for cell in row:
            if cell and str(cell).strip():
                return True
        
        return False

    def collect_results(self):
        # ä½¿ç”¨é…ç½®ä¸­çš„ model_countï¼Œå¦‚æœæœªè¨­å®šå‰‡é è¨­ç‚º1
        model_count = self._rules[0][0].get("model_count", 1)
        result_rows = [[] for _ in range(model_count)]

        for rule_index, rule in enumerate(self._rules[1]):
            keywords = rule.get("keywords", [])
            logic = rule.get("logic", "OR").upper()
            rowspan = rule.get("rowspan", 1)
            column_name = rule.get("column_name", f"æ¬„ä½{rule_index+1}")

            matched_blocks = []

            for i, row in enumerate(self.datalist):
                if logic == "AND":
                    match = True
                    for kw in keywords:
                        if not any(kw.lower() in str(cell).lower() for cell in row):
                            match = False
                            break
                else:
                    match = False
                    for kw in keywords:
                        if any(kw.lower() in str(cell).lower() for cell in row):
                            match = True
                            break

                if match:
                    block = self.datalist[i:i + rowspan]
                    matched_blocks.append((i, block))

            if len(matched_blocks) == 0:
                print(f"âš ï¸ è¦å‰‡ {rule_index+1} - {column_name}: æ‰¾ä¸åˆ°é—œéµå­— {keywords}ï¼Œå…¨æ¬„å¡«ç©ºç™½")
                for idx in range(model_count):
                    result_rows[idx].append("")
            else:
                if len(matched_blocks) > 1:
                    print(f"âš ï¸ è­¦å‘Šï¼šè¦å‰‡ {rule_index+1} - {column_name} æ‰¾åˆ°è¶…éä¸€å€‹åŒ¹é…ï¼ˆå…± {len(matched_blocks)} å€‹ï¼‰ï¼Œåƒ…ä½¿ç”¨ç¬¬ä¸€ç­†ï¼")
                first_index, block = matched_blocks[0]
                print(f"ğŸ” è¦å‰‡ {rule_index+1} - {column_name}: æ‰¾åˆ°é—œéµå­— {keywords} æ–¼ç¬¬ {first_index+1} è¡Œ")

                for idx in range(model_count):
                    col_index = 2 + idx
                    if all(col_index < len(row) for row in block):
                        lines = []
                        for r in block:
                            value = r[col_index].strip()
                            prefix_label = r[1].strip() if len(r) > 1 else ""
                            if prefix_label:
                                lines.append(f"{prefix_label}: {value}")
                            else:
                                lines.append(value)
                        result = "\n".join(lines).strip()
                        result_rows[idx].append(result)
                        print(f"  â†’ æ©Ÿç¨®{idx+1}: {result}")
                    else:
                        result_rows[idx].append("")
                        print(f"  â†’ æ©Ÿç¨®{idx+1}: âš ï¸è­¦å‘Šï¼šè©²æ©Ÿç¨®æ­¤è™•ç„¡è³‡æ–™")

        # å°‡çµæœè½‰ç‚º dict ä¸¦å­˜åˆ° processed_result
        self.processed_result = []
        for row in result_rows:
            # åªæ ¹æ“š headers çµ„è£æ¬„ä½ï¼Œä¸è‡ªå‹•åŠ  modeltype
            row_dict = {}
            for i, header in enumerate(self.headers):
                row_dict[header] = row[i] if i < len(row) else ""
            self.processed_result.append(row_dict)
    

    # def write_csv(self, output_path, result_rows, headers, model_type):
    def write_csv(self):
        if self._rules is None:
            # å‹•æ…‹æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨æ¨™é¡Œ
            headers = self.headers
        else:
            # è¦å‰‡æ¨¡å¼ï¼šæ·»åŠ  modeltype æ¬„ä½
            headers = ["modeltype"] + self.headers
        
        # å»ºç«‹è¨˜æ†¶é«”ä¸­çš„ processed_csv è³‡æ–™çµæ§‹
        self.processed_csv = []
        
        for row in self.processed_result:
            if self._rules is None:
                # å‹•æ…‹æ¨¡å¼ï¼šç›´æ¥æ˜ å°„è³‡æ–™
                row_dict = {}
                for i, header in enumerate(self.headers):
                    row_dict[header] = row[i] if i < len(row) else ""
            else:
                # è¦å‰‡æ¨¡å¼ï¼šæ·»åŠ  modeltype
                row_dict = {"modeltype": "dynamic"}
                for i, header in enumerate(self.headers):
                    row_dict[header] = row[i] if i < len(row) else ""
            self.processed_csv.append(row_dict)
        
        print(f"âœ… å·²å»ºç«‹è¨˜æ†¶é«”è³‡æ–™ï¼š{len(self.processed_csv)} ç­†è¨˜éŒ„")
        
        # ä¿æŒåŸæœ‰çš„æª”æ¡ˆè¼¸å‡ºåŠŸèƒ½
        with open(self.default_output_path, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(headers)
            for row in self.processed_result:
                writer.writerow([row[i] for i in range(len(row))])
        print(f"âœ… å·²è¼¸å‡ºè‡³ï¼š{self.default_output_path}")

    # è½‰æ› XLSX ç‚º CSV æª”æ¡ˆ cayman 20250722
    def convert_xlsx_to_csv_if_needed(self, file_path):
        if file_path.lower().endswith(".csv"):
            return file_path
        elif file_path.lower().endswith(".xlsx"):
            print(f"ğŸ” åµæ¸¬åˆ° XLSX æª”æ¡ˆï¼Œæ­£åœ¨è½‰æ›ç‚º CSVï¼š{file_path}")
            df = pd.read_excel(file_path, sheet_name=0, header=None)
            temp_csv_path = os.path.join(tempfile.gettempdir(), "_converted_temp.csv")
            df.to_csv(temp_csv_path, index=False, header=False, encoding="utf-8-sig")
            print(f"âœ… å·²è½‰æ›ç‚ºæš«å­˜ CSVï¼š{temp_csv_path}")
            return temp_csv_path
        else:
            raise ValueError("âŒ åƒ…æ”¯æ´ .csv æˆ– .xlsx æª”æ¡ˆï¼")

# def main():
#     parser = argparse.ArgumentParser(description="æ ¹æ“š JSON è¦å‰‡æœå°‹ CSV ä¸¦è½‰ç‚ºæ¯æ©Ÿç¨®ä¸€åˆ—æ ¼å¼ï¼ˆv4 + prefix æ¯åˆ—è®€å–ï¼‰")
#     parser.add_argument("csv_path", help="è¼¸å…¥CSV")
#     parser.add_argument("json_path", help="è¦å‰‡JSON")
#     parser.add_argument("--model_count", type=int, required=True, help="å¹¾å€‹æ©Ÿç¨®")
#     parser.add_argument("--model_type", type=str, required=True, help="å…±ç”¨çš„ model type å­—ä¸²")
#     parser.add_argument("--output_csv", required=True, help="è¼¸å‡ºCSVæª”å")

#     args = parser.parse_args()

#     reader = load_csv(args.csv_path)
#     rules = load_rules(args.json_path)
#     results = collect_results(reader, rules, args.model_count)

#     headers = [
#         rule.get("column_name").strip() if rule.get("column_name") and rule.get("column_name").strip() != "" else f"æ¬„ä½{i+1}"
#         for i, rule in enumerate(rules)
#     ]
#     write_csv(args.output_csv, results, headers, args.model_type)

# if __name__ == "__main__":
#     main()